{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing FIFA WWC 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "file_path = 'fifa_world_cup_2022_tweets.csv'\n",
    "tweets_df = pd.read_csv(file_path)\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def clean_tweet_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # odstraneni URL\n",
    "    text = re.sub(r'@\\w+', '', text)  # odstraneni uzivatelskych jmen\n",
    "    text = re.sub(r'#', '', text)  # odstraneni hashtagu\n",
    "    text = re.sub(r'\\n', ' ', text)  # nahrazeni novych radku mezerami\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # odstraneni nealfabetickych znaku\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # odstraneni nadbytecnych mezer\n",
    "    return text\n",
    "\n",
    "tweets_df['Cleaned Tweet'] = tweets_df['Tweet'].apply(clean_tweet_text)\n",
    "print(\"\\nData after cleaning:\")\n",
    "print(tweets_df[['Tweet', 'Cleaned Tweet']].head())\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Prevede treebank tagy na tagy WordNet.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  \n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text) # tokenizace textu\n",
    "    tagged_tokens = pos_tag(tokens)  # POS tagging\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_tokens]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "tweets_df['Tokenized and Lemmatized Tweet'] = tweets_df['Cleaned Tweet'].apply(tokenize_and_lemmatize)\n",
    "print(\"\\nData after tokenization and lemmatization:\")\n",
    "print(tweets_df[['Cleaned Tweet', 'Tokenized and Lemmatized Tweet']].head())\n",
    "\n",
    "# prevod textovych sentimentu na ciselne kody\n",
    "label_encoder = LabelEncoder()\n",
    "tweets_df['Sentiment Code'] = label_encoder.fit_transform(tweets_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordCloud for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Sentiment', data=tweets_df, palette='viridis')\n",
    "plt.title('Distribution of Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "selected_sentiments = sentiments[:3]\n",
    "\n",
    "for sentiment in selected_sentiments:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    subset = tweets_df[tweets_df['Sentiment'] == sentiment]\n",
    "    text = \" \".join(review for review in subset['Tokenized and Lemmatized Tweet'])\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=200, contour_width=3, contour_color='steelblue').generate(text)\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'WordCloud for {sentiment} Sentiment')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models without lemmatization and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# rozdeleni dat na trenovaci a testovaci\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tweets_df['Cleaned Tweet'],  \n",
    "    tweets_df['Sentiment Code'],  \n",
    "    test_size=0.2,  # velikost testovaci sady\n",
    "    random_state=42  # seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# inicializace TfidfVectorizeru\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes without lemmatization and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression without lemmatization and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_log_reg = log_reg_classifier.predict(X_test_tfidf)\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_log_reg}\")\n",
    "print(classification_report(y_test, y_pred_log_reg, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines without lemmatization and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_svm = svm_classifier.predict(X_test_tfidf)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Accuracy: {accuracy_svm}\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenizace a priprava datovych sekvenci\n",
    "encoded_data = [tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True) for text in tweets_df['Cleaned Tweet']]\n",
    "max_len = max([len(sent) for sent in encoded_data])\n",
    "input_ids = [sent + [0] * (max_len - len(sent)) for sent in encoded_data]\n",
    "\n",
    "attention_masks = [[float(i > 0) for i in seq] for seq in input_ids]\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "labels = torch.tensor(tweets_df['Sentiment Code'].values).long()\n",
    "\n",
    "# rozdeleni dat na trenovaci a validacni sadu\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.2)\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",  \n",
    "    num_labels=3,  \n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# nastaveni optimizeru a scheduleru\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "# trenovani modelu\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(f\"{'='*8} Epoch {epoch_i + 1} / {epochs} {'='*8}\")\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "        \n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Average training loss: {avg_train_loss}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = [1, 2, 3, 4]\n",
    "training_losses = [0.5781036733593851, 0.34983963131528545, 0.2245372372976143, 0.15559972962364554]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, training_losses, marker='o', linestyle='-', color='blue')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(visible=True)\n",
    "plt.xticks(epochs)  \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in validation_dataloader:\n",
    "    batch = tuple(t for t in batch)\n",
    "    \n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(\"Validation Accuracy: {:.2f}\".format(accuracy_score(true_labels, predicted_labels)))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(true_labels, predicted_labels, target_names=['Negative', 'Neutral', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  \n",
    "\n",
    "conf_mat_bert = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_mat_bert, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "plt.title('Confusion Matrix for BERT Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"/Plocha/bp \"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models with lemmatization and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# rozdeleni dat na trenovaci a testovaci\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tweets_df['Tokenized and Lemmatized Tweet'],  \n",
    "    tweets_df['Sentiment Code'],  \n",
    "    test_size=0.2,  # velikost testovaci sady\n",
    "    random_state=42  # seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes with lemmatization and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix for Naive Bayes')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression without lemmatization and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_classifier.fit(X_train_tfidf, y_train)\n",
    "y_pred_log_reg = log_reg_classifier.predict(X_test_tfidf)\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "print(f\"Accuracy: {accuracy_log_reg}\")\n",
    "print(classification_report(y_test, y_pred_log_reg, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_mat_log_reg = confusion_matrix(y_test, y_pred_log_reg)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_mat_log_reg, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines without lemmatization and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_svm = svm_classifier.predict(X_test_tfidf)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Accuracy: {accuracy_svm}\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_mat_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_mat_svm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix for SVM Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing WWC23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_paths = [\n",
    "    'dataset_tweet-scraper_2024-03-27_20-40-06-746.csv',\n",
    "    'dataset_tweet-scraper_2024-03-27_20-39-24-804.csv',\n",
    "    'dataset_tweet-scraper_2024-03-27_20-38-14-709.csv',\n",
    "    'dataset_tweet-scraper_2024-03-27_20-37-19-059.csv',\n",
    "    'dataset_tweet-scraper_2024-03-27_20-36-29-379.csv',\n",
    "    'dataset_tweet-scraper_2024-03-27_20-35-46-867.csv',\n",
    "    'dataset_tweet-scraper_2024-03-27_20-35-15-063.csv',\n",
    "    'dataset_tweet-scraper_2024-03-27_20-34-05-633.csv',\n",
    "    'dataset_tweet-scraper_2024-03-25_21-20-38-158.csv',\n",
    "    'dataset_tweet-scraper_2024-03-25_20-55-59-951.csv',\n",
    "]\n",
    "\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# detekce sloupce s textem tweetu\n",
    "if \"text\" not in merged_df.columns:\n",
    "    for col in merged_df.columns:\n",
    "        if merged_df[col].dtype == \"object\":\n",
    "            text_column = col\n",
    "            break\n",
    "else:\n",
    "    text_column = \"text\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # konverze textu na mala pismena\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)     # odstraneni URL\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)     # odstraneni uzivatelskych jmen a hashtagu\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)     # odstraneni speciálních znaků\n",
    "    stop_words = set(stopwords.words('english'))        # filtrace stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "merged_df[text_column + '_preprocessed'] = merged_df[text_column].apply(preprocess_text)\n",
    "merged_df[[text_column, text_column + '_preprocessed']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding sentiments to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def sentiment_analysis(tweet_df):\n",
    "    def getSubjectivity(text):\n",
    "        return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "    def getPolarity(text):\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    \n",
    "    # aplikace funkci pro vypocet subjektivity a polarity\n",
    "    tweet_df['TextBlob_Subjectivity'] = tweet_df[text_column + '_preprocessed'].apply(getSubjectivity)\n",
    "    tweet_df['TextBlob_Polarity'] = tweet_df[text_column + '_preprocessed'].apply(getPolarity)\n",
    "\n",
    "    # prirazeni sentimentu na zaklade polarity\n",
    "    def getAnalysis(score):\n",
    "        if score < 0:\n",
    "            return 'Negative'\n",
    "        elif score == 0:\n",
    "            return 'Neutral'\n",
    "        else:\n",
    "            return 'Positive'\n",
    "    \n",
    "    tweet_df['TextBlob_Analysis'] = tweet_df['TextBlob_Polarity'].apply(getAnalysis)\n",
    "    return tweet_df\n",
    "\n",
    "merged_df = sentiment_analysis(merged_df)\n",
    "\n",
    "# prevod textovych labelu sentimentu na numericke kody\n",
    "label_encoder = LabelEncoder()\n",
    "merged_df['Sentiment Code'] = label_encoder.fit_transform(merged_df['TextBlob_Analysis'])\n",
    "\n",
    "merged_df[[text_column, text_column + '_preprocessed', 'TextBlob_Analysis', 'Sentiment Code']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying trained BERT on WWC23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report, accuracy_score \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "model_path = \"/Plocha/bp\"\n",
    "\n",
    "# Nacteni tokenizeru a modelu BERT z lokalniho uloziste\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def encode_texts(texts):\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    return encoding\n",
    "\n",
    "# priprava dat\n",
    "texts = merged_df[text_column + '_preprocessed'].tolist()\n",
    "labels = merged_df['Sentiment Code'].tolist()  \n",
    "\n",
    "encoded_texts = encode_texts(texts)\n",
    "input_ids = encoded_texts['input_ids']\n",
    "attention_masks = encoded_texts['attention_mask']\n",
    "\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "data_loader = DataLoader(dataset, batch_size=32)  \n",
    "\n",
    "def predict(data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            predictions.append(logits)\n",
    "            real_values.append(label_ids)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    real_values = np.concatenate(real_values, axis=0)\n",
    "    return predictions, real_values\n",
    "\n",
    "# provedeni predikce\n",
    "predictions, real_values = predict(data_loader)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(real_values, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(real_values, predicted_labels, target_names=['Negative', 'Neutral', 'Positive'])\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
